<template>
  <div class="editor">
    <div class="editor-container">
      <!-- é¡¶éƒ¨å·¥å…·æ  -->
      <EditorToolbar
        :workflow-stage="workflowStage"
        :has-tracks="hasTracks"
        @stage-change="handleStageChange"
        @save-project="saveProject"
        @export-project="exportProject"
      />

      <!-- æ–‡æœ¬è¾“å…¥é˜¶æ®µ -->
      <TextInputStage
        v-if="workflowStage === 'input'"
        ref="textInputStageRef"
        @start-segmentation="handleStartSegmentation"
        @segmentation-confirm="handleSegmentationConfirm"
      />

      <!-- æ—¶é—´è½´ç¼–è¾‘é˜¶æ®µ -->
      <TimelineEditStage
        v-else-if="workflowStage === 'timeline'"
        :timeline-segments="timelineSegments"
        :timeline-gaps="timelineGaps"
        :total-timeline-duration="totalTimelineDuration"
        :selected-segment-id="selectedSegmentId"
        :playing-segment-id="playingSegmentId"
        :selected-gap-id="selectedGapId"
        :show-voice-editor="showSentenceVoiceEditor"
        :selected-segment-for-voice="selectedSegmentForVoice"
        :selected-segment-index="selectedSegmentIndex"
        @select-segment="handleSelectSegment"
        @play-segment="handlePlaySegment"
        @edit-segment="handleEditSegment"
        @add-text="handleAddText"
        @update-segment-text="handleUpdateSegmentText"
        @delete-segment="handleDeleteSegment"
        @add-sentence-after="handleAddSentenceAfter"
        @add-gap="handleAddGap"
        @select-gap="handleSelectGap"
        @update-gap-duration="handleUpdateGapDuration"
        @remove-gap="handleRemoveGap"
        @open-voice-editor="handleOpenVoiceEditor"
        @synthesize-audio="handleSynthesizeAudio"
        @play-audio="handlePlayAudio"
        @ssml-update="handleSSMLUpdate"
        @voice-update="handleVoiceUpdate"
        @volume-update="handleVolumeUpdate"
        @speed-update="handleSpeedUpdate"
        @pitch-update="handlePitchUpdate"
        @update-pause-marks="handlePauseMarksUpdate"
        @update-pronunciation-marks="handlePronunciationMarksUpdate"
        @close-voice-editor="closeSentenceVoiceEditor"
      />
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, computed, onMounted, watch } from 'vue'
import { ElMessage } from 'element-plus'
import { storeToRefs } from 'pinia'
import { useAudioStore, useProjectStore, useAppStore } from '../stores'
import { TextSegmentation } from '../utils/textSegmentation'

// å¯¼å…¥æ¨¡å—åŒ–ç»„ä»¶
import EditorToolbar from '@/modules/editor/components/EditorToolbar.vue'
import TextInputStage from '@/modules/editor/components/TextInputStage.vue'
import TimelineEditStage from '@/modules/editor/components/TimelineEditStage.vue'

const audioStore = useAudioStore()
const projectStore = useProjectStore()
const appStore = useAppStore()

// ä½¿ç”¨ storeToRefs è·å–å“åº”å¼æ•°æ®
const { tracks, currentTrackId, hasTracks, currentTrack } = storeToRefs(audioStore)

// å“åº”å¼æ•°æ®
const workflowStage = ref<'input' | 'timeline'>('input')
const selectedSegmentId = ref<string>('')
const selectedSegmentIndex = ref<number>(-1)
const playingSegmentId = ref<string>('')
const selectedGapId = ref<string>('')
const showSentenceVoiceEditor = ref<boolean>(false)
const selectedSegmentForVoice = ref<any>(null)

// ç»„ä»¶å¼•ç”¨
const textInputStageRef = ref()

// Storeè§£æ„
const {
  addTrack,
  removeTrack,
  setCurrentTrack,
  playTrack,
  clearTracks,
  segmentTrack,
  unsegmentTrack,
  updateSegmentText,
  removeSegment,
  addSegment,
  addGap,
  updateGapDuration,
  removeGap,
  selectGap,
  initializeGaps
} = audioStore

const {
  currentProject,
  saveProject: saveProjectToStore
} = projectStore

// è®¡ç®—å±æ€§
const timelineSegments = computed(() => {
  const segments: any[] = []
  let currentTime = 0

  tracks.value.forEach(track => {
    if (track.isSegmented && track.segments) {
      track.segments.forEach((segment, index) => {
        const segmentDuration = (segment as any).duration || 3
        segments.push({
          ...segment,
          startTime: currentTime,
          endTime: currentTime + segmentDuration,
          duration: segmentDuration,
          isPlaying: track.isPlaying,
          audioUrl: track.audioUrl,
          voice: track.voice,
          speed: track.speed,
          pitch: track.pitch
        })
        currentTime += segmentDuration
      })
    } else {
      segments.push({
        id: track.id,
        text: track.text,
        startTime: currentTime,
        endTime: currentTime + (track.duration || 0),
        duration: track.duration || 0,
        isPlaying: track.isPlaying,
        audioUrl: track.audioUrl,
        voice: track.voice,
        speed: track.speed,
        pitch: track.pitch,
        startIndex: 0,
        endIndex: track.text.length,
        type: 'sentence'
      })
      currentTime += track.duration || 0
    }
  })

  return segments
})

const timelineGaps = computed(() => {
  const gaps: any[] = []
  let currentTime = 0

  tracks.value.forEach(track => {
    if (track.isSegmented && track.segments && track.gaps) {
      track.gaps.forEach((gap, index) => {
        const beforeSegment = track.segments?.find(s => s.id === gap.beforeSegmentId)
        const afterSegment = track.segments?.find(s => s.id === gap.afterSegmentId)

        if (beforeSegment && afterSegment) {
          const gapStartTime = currentTime + ((beforeSegment as any).duration || 3)

          gaps.push({
            ...gap,
            startTime: gapStartTime,
            endTime: gapStartTime + gap.duration
          })
        }
      })
    }
  })

  return gaps
})

const totalTimelineDuration = computed(() => {
  const duration = timelineSegments.value.reduce((total, segment) => total + segment.duration, 0)
  const gapDuration = timelineGaps.value.reduce((total, gap) => total + gap.duration, 0)
  return duration + gapDuration
})

// äº‹ä»¶å¤„ç†æ–¹æ³•
const handleStageChange = (stage: 'input' | 'timeline') => {
  workflowStage.value = stage
}

const saveProject = async () => {
  if (!currentProject) {
    ElMessage.warning('è¯·å…ˆåˆ›å»ºæˆ–é€‰æ‹©é¡¹ç›®')
    return
  }

  try {
    await saveProjectToStore(currentProject)
    ElMessage.success('é¡¹ç›®ä¿å­˜æˆåŠŸ')
  } catch (error) {
    console.error('ä¿å­˜é¡¹ç›®å¤±è´¥:', error)
    ElMessage.error('ä¿å­˜é¡¹ç›®å¤±è´¥')
  }
}

const exportProject = async () => {
  if (!currentProject) {
    ElMessage.warning('è¯·å…ˆåˆ›å»ºæˆ–é€‰æ‹©é¡¹ç›®')
    return
  }

  try {
    const data = await projectStore.exportProject(currentProject.id)
    const blob = new Blob([JSON.stringify(data)], { type: 'application/json' })
    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    a.download = `${currentProject.name}.json`
    a.click()
    URL.revokeObjectURL(url)

    ElMessage.success('é¡¹ç›®å¯¼å‡ºæˆåŠŸ')
  } catch (error) {
    console.error('å¯¼å‡ºé¡¹ç›®å¤±è´¥:', error)
    ElMessage.error('å¯¼å‡ºé¡¹ç›®å¤±è´¥')
  }
}

const handleStartSegmentation = (text: string) => {
  // æ–‡æœ¬å·²ç»åœ¨TextInputStageä¸­å¤„ç†ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–æ“ä½œ
}

const handleSegmentationConfirm = (method: 'punctuation' | 'paragraph' | 'ai', text: string) => {
  performSegmentation(text, method)
}

const performSegmentation = (text: string, method: 'punctuation' | 'paragraph' | 'ai') => {
  try {
    const segments = TextSegmentation.segment(text, {
      method: method,
      minLength: 1,
      maxLength: 100,
      preserveWhitespace: true
    })

    if (segments.length === 0) {
      ElMessage.warning('åˆ†å¥ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡æœ¬å†…å®¹')
      return
    }

    const segmentsWithDuration = segments.map((segment, index) => ({
      ...segment,
      duration: 3,
      voice: 'zhichu',  // åˆå§‹åŒ–è¯­éŸ³è®¾ç½®
      speed: 1.0,
      pitch: 1.0,
      volume: 1.0,
      pauseMarks: [],  // åˆå§‹åŒ–åœé¡¿æ ‡è®°
      pronunciationMarks: []  // åˆå§‹åŒ–å‘éŸ³æ ‡è®°
    }))

    clearTracks()

    const trackId = addTrack({
      name: `åˆ†å¥æ–‡æœ¬_${Date.now()}`,
      text: text,
      voice: 'zhichu',
      speed: 1.0,
      pitch: 1.0,
      volume: 1.0,
      audioUrl: undefined,
      duration: segmentsWithDuration.reduce((total, seg) => total + seg.duration, 0)
    })

    segmentTrack(trackId, segmentsWithDuration, method)

    // ç¡®ä¿åœ¨segmentTrackå®Œæˆåç«‹å³åˆå§‹åŒ–gaps
    if (segments.length > 1) {
      audioStore.initializeGaps(trackId)
    }

    ElMessage.success(`æˆåŠŸåˆ†å¥ä¸º ${segments.length} ä¸ªå¥å­`)
    workflowStage.value = 'timeline'

    if (segmentsWithDuration.length > 0) {
      const firstSegment = segmentsWithDuration[0]
      selectedSegmentId.value = firstSegment.id
      selectedSegmentIndex.value = 0
      handleOpenVoiceEditor(firstSegment)
    }

  } catch (error) {
    console.error('åˆ†å¥å¤„ç†å¤±è´¥:', error)
    ElMessage.error('åˆ†å¥å¤„ç†å¤±è´¥')
  }
}

// æ—¶é—´è½´ç›¸å…³äº‹ä»¶å¤„ç†
const handleSelectSegment = (segment: any) => {
  selectedSegmentId.value = segment.id
  selectedSegmentIndex.value = timelineSegments.value.findIndex(s => s.id === segment.id)

  // è‡ªåŠ¨æ‰“å¼€è¯­éŸ³ç¼–è¾‘å™¨
  handleOpenVoiceEditor(segment)
}

const handlePlaySegment = (segmentId: string) => {
  const segment = timelineSegments.value.find(s => s.id === segmentId)
  if (segment) {
    playingSegmentId.value = segmentId
    ElMessage.info(`æ’­æ”¾åˆ†å¥: ${segment.text}`)
  }
}

const handleEditSegment = (segmentId: string) => {
  const segment = timelineSegments.value.find(s => s.id === segmentId)
  if (segment) {
    handleSelectSegment(segment)
  }
}

const handleAddText = (position: number) => {
  ElMessage.info(`åœ¨ä½ç½® ${position.toFixed(1)}s æ·»åŠ æ–‡å­—åŠŸèƒ½å¼€å‘ä¸­...`)
}

const handleUpdateSegmentText = (segmentId: string, newText: string) => {

  // æ‰¾åˆ°åŒ…å«æ­¤segmentçš„track
  const track = tracks.value.find(t => t.isSegmented && t.segments)
  if (track && track.segments) {
    const segment = track.segments.find(s => s.id === segmentId)
    if (segment) {
      // ä½¿ç”¨storeæ–¹æ³•æ¥æ›´æ–°æ–‡æœ¬ï¼Œç¡®ä¿å“åº”å¼æ›´æ–°
      audioStore.updateSegmentText(track.id, segmentId, newText)

      // å¦‚æœå½“å‰ç¼–è¾‘çš„æ˜¯é€‰ä¸­çš„è¯­éŸ³ç¼–è¾‘å™¨ä¸­çš„å¥å­ï¼Œä¹Ÿè¦æ›´æ–°å®ƒ
      if (selectedSegmentForVoice.value && selectedSegmentForVoice.value.id === segmentId) {
        selectedSegmentForVoice.value.text = newText
      }

      ElMessage.success('å¥å­å†…å®¹å·²æ›´æ–°')
    } else {
      console.error('Editor - segment not found:', segmentId)
    }
  } else {
    console.error('Editor - track or segments not found')
  }
}

const handleDeleteSegment = (segmentId: string) => {
  const track = tracks.value.find(t => t.isSegmented && t.segments)
  if (track && track.segments) {
    const index = track.segments.findIndex(s => s.id === segmentId)
    if (index > -1) {
      track.segments.splice(index, 1)
      track.text = track.segments.map(seg => seg.text).join(' ')

      if (selectedSegmentId.value === segmentId) {
        selectedSegmentId.value = ''
        selectedSegmentIndex.value = -1
      }

      ElMessage.success('å¥å­å·²åˆ é™¤')

      if (track.segments.length === 0) {
        workflowStage.value = 'input'
        ElMessage.info('æ‰€æœ‰å¥å­å·²åˆ é™¤ï¼Œè¿”å›æ–‡æœ¬è¾“å…¥é˜¶æ®µ')
      }
    }
  }
}

const handleAddSentenceAfter = (segmentId: string, index: number) => {

  const track = tracks.value.find(t => t.isSegmented && t.segments)
  if (track && track.segments) {
    track.segments.forEach((seg, idx) => {
    })

    // ç¡®è®¤å½“å‰é€‰ä¸­çš„segmentçš„å®é™…ä½ç½®
    const actualIndex = track.segments.findIndex(seg => seg.id === segmentId)

    // æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŸæ¥çš„é—´éš”éœ€è¦å¤„ç†
    let originalGapToRemove = null
    if (actualIndex < track.segments.length - 1) {
      const nextSegment = track.segments[actualIndex + 1]
      if (nextSegment && track.gaps) {
        // æŸ¥æ‰¾åŸæ¥çš„é—´éš”
        originalGapToRemove = track.gaps.find(gap =>
          gap.beforeSegmentId === segmentId && gap.afterSegmentId === nextSegment.id
        )
        if (originalGapToRemove) {
        }
      }
    }

    const newSegment = {
      id: `segment_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      text: '',
      startIndex: 0,
      endIndex: 0,
      type: 'sentence' as const,
      voice: 'zhichu',  // åˆå§‹åŒ–è¯­éŸ³è®¾ç½®
      speed: 1.0,
      pitch: 1.0,
      volume: 1.0,
      pauseMarks: [],  // åˆå§‹åŒ–åœé¡¿æ ‡è®°
      pronunciationMarks: []  // åˆå§‹åŒ–å‘éŸ³æ ‡è®°
    }

    // ä½¿ç”¨å®é™…æ‰¾åˆ°çš„ç´¢å¼•ä½ç½®ï¼Œåœ¨å…¶åé¢æ’å…¥
    const insertIndex = actualIndex + 1

    track.segments.splice(insertIndex, 0, newSegment)

    track.segments.forEach((seg, idx) => {
    })

    // å¤„ç†é—´éš”ï¼š
    // 1. å¦‚æœå­˜åœ¨åŸæ¥çš„é—´éš”ï¼Œå…ˆåˆ é™¤å®ƒ
    if (originalGapToRemove && track.gaps) {
      const gapIndex = track.gaps.findIndex(g => g.id === originalGapToRemove.id)
      if (gapIndex > -1) {
        track.gaps.splice(gapIndex, 1)
      }
    }

    // 2. åˆ›å»ºæ–°çš„é—´éš”
    // åˆ›å»º "å‰ä¸€å¥ -> æ–°å¥å­" çš„é—´éš”
    const prevSegment = track.segments[insertIndex - 1]
    if (prevSegment) {
      audioStore.addGap(track.id, prevSegment.id, newSegment.id, 1)
    }

    // åˆ›å»º "æ–°å¥å­ -> åä¸€å¥" çš„é—´éš”
    if (insertIndex < track.segments.length - 1) {
      const nextSegment = track.segments[insertIndex + 1]
      if (nextSegment) {
        audioStore.addGap(track.id, newSegment.id, nextSegment.id, 1)
      }
    }

    selectedSegmentId.value = newSegment.id

    // æ‰“å°æœ€ç»ˆçš„æ—¶é—´è½´æ•°æ®
    track.segments.forEach((seg, idx) => {
    })

    if (track.gaps) {
      track.gaps.forEach((gap, idx) => {
      })
    }

    const finalTimelineSegments = timelineSegments.value
    finalTimelineSegments.forEach((seg, idx) => {
    })

    ElMessage.success('æ–°å¥å­å·²æ·»åŠ ï¼Œè¯·ç¼–è¾‘å†…å®¹')
  } else {
    console.error('Track not found or no segments')
  }
}

const handleAddGap = (beforeSegmentId: string, afterSegmentId: string) => {

  const track = tracks.value.find(t => t.isSegmented && t.segments)
  if (track && track.segments) {

    // æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨é—´éš”
    const existingGap = track.gaps?.find(gap => gap.beforeSegmentId === beforeSegmentId)
    if (existingGap) {
      ElMessage.warning('è¿™ä¸¤ä¸ªå¥å­ä¹‹é—´å·²ç»å­˜åœ¨é—´éš”')
      return
    }

    // åˆ›å»ºæ–°çš„é—´éš”ï¼Œé»˜è®¤1ç§’
    audioStore.addGap(track.id, beforeSegmentId, afterSegmentId, 1)
    ElMessage.success('å·²æ·»åŠ 1ç§’é—´éš”')
  } else {
    console.error('Track not found or no segments')
    ElMessage.error('æ·»åŠ é—´éš”å¤±è´¥ï¼šæœªæ‰¾åˆ°å¯¹åº”çš„è½¨é“æ•°æ®')
  }

}

// é—´éš”ç›¸å…³äº‹ä»¶å¤„ç†
const handleSelectGap = (gap: any) => {
  selectedGapId.value = gap.id
  selectedSegmentId.value = ''
  selectedSegmentIndex.value = -1
}

const handleUpdateGapDuration = (gapId: string, duration: number) => {

  const track = tracks.value.find(t => t.isSegmented && t.gaps)

  if (track && track.gaps) {
    const gap = track.gaps.find(g => g.id === gapId)

    if (gap) {
      audioStore.updateGapDuration(track.id, gapId, duration)
    } else {
      console.error('Gap not found with gapId:', gapId)
    }
  } else {
    console.error('Track not found or has no gaps')
  }

}

const handleRemoveGap = (gapId: string) => {

  const track = tracks.value.find(t => t.isSegmented && t.gaps)

  if (track && track.gaps) {

    // è°ƒç”¨storeæ–¹æ³•åˆ é™¤gap
    audioStore.removeGap(track.id, gapId)

    // æ¸…ç†é€‰ä¸­çŠ¶æ€
    if (selectedGapId.value === gapId) {
      selectedGapId.value = ''
    }

    // æ¸…ç†è¯­éŸ³ç¼–è¾‘å™¨çŠ¶æ€ï¼ˆå¦‚æœå½“å‰æ­£åœ¨ç¼–è¾‘ï¼‰
    if (showSentenceVoiceEditor.value) {
      showSentenceVoiceEditor.value = false
      selectedSegmentForVoice.value = null
    }

    ElMessage.success('é—´éš”å·²åˆ é™¤')
  } else {
    console.error('Track not found or has no gaps')
    ElMessage.error('åˆ é™¤é—´éš”å¤±è´¥ï¼šæœªæ‰¾åˆ°å¯¹åº”çš„è½¨é“æ•°æ®')
  }

}

// è¯­éŸ³ç¼–è¾‘ç›¸å…³äº‹ä»¶å¤„ç†

const handleOpenVoiceEditor = (segment: any) => {
  console.log('=== Editor - handleOpenVoiceEditor START ===')
  console.log('segment:', segment.id, segment.text)
  
  // ä»storeä¸­è·å–å·²ä¿å­˜çš„è¯­éŸ³è®¾ç½®
  const track = tracks.value.find(t => t.isSegmented && t.segments)
  console.log('found track:', track?.id)
  
  if (track) {
    console.log('Editor - calling audioStore.getSegmentVoiceSettings with:', track.id, segment.id)
    
    const savedSettings = audioStore.getSegmentVoiceSettings(track.id, segment.id)
    console.log('Editor - savedSettings from store:', savedSettings)
    console.log('savedSettings type:', typeof savedSettings)
    console.log('savedSettings is null:', savedSettings === null)
    if (savedSettings) {
      console.log('savedSettings.pauseMarks:', savedSettings.pauseMarks)
      console.log('savedSettings.pronunciationMarks:', savedSettings.pronunciationMarks)
    }
    
    // åˆ›å»ºå¸¦æœ‰é»˜è®¤å€¼çš„segmentå¯¹è±¡
    const createSegmentWithDefaults = (baseSegment: any, savedSettings?: any) => ({
      ...baseSegment,
      voice: savedSettings?.voice || baseSegment.voice || 'zhichu',
      speed: savedSettings?.speed ?? baseSegment.speed ?? 1,
      pitch: savedSettings?.pitch ?? baseSegment.pitch ?? 1,
      volume: savedSettings?.volume ?? baseSegment.volume ?? 1,
      ssml: savedSettings?.ssml || baseSegment.ssml || '',
      pauseMarks: savedSettings?.pauseMarks || baseSegment.pauseMarks || [],
      pronunciationMarks: savedSettings?.pronunciationMarks || baseSegment.pronunciationMarks || []
    })

    if (savedSettings) {
      console.log('Editor - åˆå¹¶ä¿å­˜çš„è®¾ç½®åˆ°segmentå¯¹è±¡')
      selectedSegmentForVoice.value = createSegmentWithDefaults(segment, savedSettings)
    } else {
      console.log('Editor - ä½¿ç”¨åŸå§‹segmentå¹¶æ·»åŠ é»˜è®¤å€¼')
      selectedSegmentForVoice.value = createSegmentWithDefaults(segment)
    }
    
    console.log('Editor - åˆå§‹åŒ–åçš„selectedSegmentForVoice:', selectedSegmentForVoice.value)
    console.log('Editor - pauseMarks in selectedSegmentForVoice:', selectedSegmentForVoice.value.pauseMarks)
    console.log('Editor - pronunciationMarks in selectedSegmentForVoice:', selectedSegmentForVoice.value.pronunciationMarks)
  } else {
    console.log('Editor - æ²¡æœ‰æ‰¾åˆ°track')
    selectedSegmentForVoice.value = segment
  }
  showSentenceVoiceEditor.value = true
  console.log('=== Editor - handleOpenVoiceEditor END ===')
}

const closeSentenceVoiceEditor = () => {
  showSentenceVoiceEditor.value = false
  selectedSegmentForVoice.value = null
}

const handleSynthesizeAudio = (segment: any) => {
  ElMessage.info(`æ­£åœ¨ä¸ºå¥å­"${segment.text}"åˆæˆéŸ³é¢‘...`)
}

const handlePlayAudio = (segment: any) => {
  if (segment.audioUrl) {
    const audio = new Audio(segment.audioUrl)
    audio.play().catch(error => {
      console.error('æ’­æ”¾éŸ³é¢‘å¤±è´¥:', error)
      ElMessage.error('æ’­æ”¾éŸ³é¢‘å¤±è´¥')
    })
  } else {
    ElMessage.warning('è¯¥å¥å­è¿˜æ²¡æœ‰åˆæˆéŸ³é¢‘')
  }
}

// ç»Ÿä¸€çš„è¯­éŸ³è®¾ç½®ä¿å­˜æ–¹æ³•
const saveCurrentVoiceSettings = () => {
  if (!selectedSegmentForVoice.value) return
  
  const track = tracks.value.find(t => t.isSegmented && t.segments)
  if (!track) return
  
  // æ”¶é›†æ‰€æœ‰è¯­éŸ³è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ç¡®ä¿å®Œæ•´æ€§
  const allSettings = {
    voice: selectedSegmentForVoice.value.voice || 'zhichu',
    speed: selectedSegmentForVoice.value.speed ?? 1,
    pitch: selectedSegmentForVoice.value.pitch ?? 1,
    volume: selectedSegmentForVoice.value.volume ?? 1,
    ssml: selectedSegmentForVoice.value.ssml || '',
    pauseMarks: selectedSegmentForVoice.value.pauseMarks || [],
    pronunciationMarks: selectedSegmentForVoice.value.pronunciationMarks || []
  }
  
  console.log('Editor - saving ALL voice settings to store:', allSettings)
  audioStore.updateSegmentVoiceSettings(track.id, selectedSegmentForVoice.value.id, allSettings)
}

const handleSSMLUpdate = (ssml: string) => {
  console.log('=== Editor - handleSSMLUpdate START ===')
  console.log('ssml received:', ssml)
  console.log('selectedSegmentForVoice:', selectedSegmentForVoice.value?.id)
  
  if (selectedSegmentForVoice.value) {
    selectedSegmentForVoice.value.ssml = ssml
    // ä½¿ç”¨ç»Ÿä¸€ä¿å­˜æ–¹æ³•
    saveCurrentVoiceSettings()
  }
  console.log('=== Editor - handleSSMLUpdate END ===')
}

const handleVoiceUpdate = (voice: any) => {
  if (selectedSegmentForVoice.value) {
    selectedSegmentForVoice.value.voice = voice.id
    saveCurrentVoiceSettings()
  }
}

const handleVolumeUpdate = (volume: number) => {
  if (selectedSegmentForVoice.value) {
    selectedSegmentForVoice.value.volume = volume
    saveCurrentVoiceSettings()
  }
}

const handleSpeedUpdate = (speed: number) => {
  if (selectedSegmentForVoice.value) {
    selectedSegmentForVoice.value.speed = speed
    saveCurrentVoiceSettings()
  }
}

const handlePitchUpdate = (pitch: number) => {
  if (selectedSegmentForVoice.value) {
    selectedSegmentForVoice.value.pitch = pitch
    saveCurrentVoiceSettings()
  }
}

const handlePauseMarksUpdate = (pauseMarks: any[]) => {
  if (selectedSegmentForVoice.value) {
    selectedSegmentForVoice.value.pauseMarks = pauseMarks
    saveCurrentVoiceSettings()
  }
}

const handlePronunciationMarksUpdate = (pronunciationMarks: any[]) => {
  if (selectedSegmentForVoice.value) {
    selectedSegmentForVoice.value.pronunciationMarks = pronunciationMarks
    saveCurrentVoiceSettings()
  }
}

// ç›‘æ§trackså˜åŒ–
watch(tracks, (newTracks) => {

  if (newTracks.length > 0 && !selectedSegmentId.value && timelineSegments.value.length > 0) {
    const firstSegment = timelineSegments.value[0]
    if (firstSegment && firstSegment.id) {
      selectedSegmentId.value = firstSegment.id
      selectedSegmentIndex.value = 0
      handleOpenVoiceEditor(firstSegment)
    }
  }

  // æ£€æŸ¥æ–°tracksæ˜¯å¦éœ€è¦åˆå§‹åŒ–gaps
  newTracks.forEach((track, index) => {
    if (track.isSegmented && track.segments && track.segments.length > 1) {

      if (!track.gaps || track.gaps.length === 0) {
        audioStore.initializeGaps(track.id)
      } else {
        // æ£€æŸ¥ç°æœ‰gapsæ˜¯å¦æœ‰NaNå€¼
        let hasNaN = false
        track.gaps.forEach((gap, gapIndex) => {
          if (isNaN(gap.duration)) {
            gap.duration = 1
            hasNaN = true
          }
        })
        if (hasNaN) {
        }
      }
    }
  })

}, { immediate: true, deep: true })

// ç”Ÿå‘½å‘¨æœŸ
onMounted(() => {
  appStore.init()

  // AudioStore æµ‹è¯•åŠŸèƒ½
  console.log('ğŸ”§ AudioStore æµ‹è¯•å¼€å§‹...')

  // æš´éœ²AudioStoreåˆ°å…¨å±€windowå¯¹è±¡ä¾›æµ‹è¯•ä½¿ç”¨
  ;(window as any).audioStore = audioStore
  ;(window as any).testVoiceSettingsFlow = () => {
    console.log('ğŸ§ª å¼€å§‹è¯­éŸ³è®¾ç½®æµç¨‹æµ‹è¯•...')

    // æµ‹è¯•å®Œæ•´çš„è¯­éŸ³è®¾ç½®ä¿å­˜å’Œæ¢å¤æµç¨‹
    if (tracks.value.length > 0) {
      const track = tracks.value[0]
      console.log('ğŸ“¦ ä½¿ç”¨track:', track.id)

      if (track.isSegmented && track.segments && track.segments.length >= 2) {
        console.log('ğŸ¯ æµ‹è¯•ä¸¤ä¸ªsegmentä¹‹é—´çš„è®¾ç½®ä¿å­˜å’Œæ¢å¤...')

        // è·å–å‰ä¸¤ä¸ªsegment
        const segment1 = track.segments[0]
        const segment2 = track.segments[1]

        console.log('ğŸ“ Segment 1:', segment1.id, segment1.text)
        console.log('ğŸ“ Segment 2:', segment2.id, segment2.text)

        // 1. ä¸ºsegment1è®¾ç½®ç‰¹å®šçš„è¯­éŸ³å‚æ•°
        const settings1 = {
          voice: 'caocao',
          speed: 1.5,
          volume: 0.8,
          pitch: 1.2,
          pauseMarks: [{ charIndex: 2, duration: 1.5 }],
           pronunciationMarks: [{ charIndex: 4, pinyin: 'test1' }]
        }

        console.log('ğŸ’¾ ä¸ºsegment1ä¿å­˜è®¾ç½®:', settings1)
        audioStore.updateSegmentVoiceSettings(track.id, segment1.id, settings1)

        // 2. éªŒè¯è®¾ç½®æ˜¯å¦ä¿å­˜æˆåŠŸ
        const retrieved1 = audioStore.getSegmentVoiceSettings(track.id, segment1.id)
        console.log('ğŸ” ä»storeè·å–segment1è®¾ç½®:', retrieved1)

        // 3. ä¸ºsegment2è®¾ç½®ä¸åŒçš„è¯­éŸ³å‚æ•°
        const settings2 = {
          voice: 'zhichu',
          speed: 0.8,
          volume: 1.2,
          pitch: 0.9,
          pauseMarks: [{ charIndex: 1, duration: 2.0 }],
           pronunciationMarks: [{ charIndex: 3, pinyin: 'test2' }]
        }

        console.log('ğŸ’¾ ä¸ºsegment2ä¿å­˜è®¾ç½®:', settings2)
        audioStore.updateSegmentVoiceSettings(track.id, segment2.id, settings2)

        // 4. éªŒè¯segment2è®¾ç½®æ˜¯å¦ä¿å­˜æˆåŠŸ
        const retrieved2 = audioStore.getSegmentVoiceSettings(track.id, segment2.id)
        console.log('ğŸ” ä»storeè·å–segment2è®¾ç½®:', retrieved2)

        // 5. å†æ¬¡è·å–segment1çš„è®¾ç½®ï¼Œç¡®è®¤æ²¡æœ‰è¢«è¦†ç›–
        const retrieved1Again = audioStore.getSegmentVoiceSettings(track.id, segment1.id)
        console.log('ğŸ” å†æ¬¡ä»storeè·å–segment1è®¾ç½®:', retrieved1Again)

         // 6. æ¯”è¾ƒè®¾ç½®æ˜¯å¦æ­£ç¡®ä¿æŒ
         const segment1Match = retrieved1Again ? JSON.stringify(settings1) === JSON.stringify({
           voice: retrieved1Again.voice,
           speed: retrieved1Again.speed,
           volume: retrieved1Again.volume,
           pitch: retrieved1Again.pitch,
           pauseMarks: retrieved1Again.pauseMarks,
           pronunciationMarks: retrieved1Again.pronunciationMarks
         }) : false

         const segment2Match = retrieved2 ? JSON.stringify(settings2) === JSON.stringify({
           voice: retrieved2.voice,
           speed: retrieved2.speed,
           volume: retrieved2.volume,
           pitch: retrieved2.pitch,
           pauseMarks: retrieved2.pauseMarks,
           pronunciationMarks: retrieved2.pronunciationMarks
         }) : false

        console.log(`ğŸ“Š Segment1è®¾ç½®ä¿æŒ: ${segment1Match ? 'âœ…' : 'âŒ'}`)
        console.log(`ğŸ“Š Segment2è®¾ç½®ä¿æŒ: ${segment2Match ? 'âœ…' : 'âŒ'}`)

        return {
          success: segment1Match && segment2Match,
          segment1: { saved: settings1, retrieved: retrieved1Again },
          segment2: { saved: settings2, retrieved: retrieved2 }
        }
      } else {
        console.log('âš ï¸ éœ€è¦è‡³å°‘2ä¸ªåˆ†æ®µæ¥æµ‹è¯•åˆ‡æ¢')
        return { success: false, reason: 'éœ€è¦è‡³å°‘2ä¸ªåˆ†æ®µ' }
      }
    } else {
      console.log('âš ï¸ æ²¡æœ‰trackæ•°æ®')
      return { success: false, reason: 'æ²¡æœ‰trackæ•°æ®' }
    }
  }

  ;(window as any).testAudioStore = () => {
    console.log('ğŸ§ª å¼€å§‹AudioStoreåŠŸèƒ½æµ‹è¯•...')

    // æµ‹è¯•1: æ£€æŸ¥storeæ–¹æ³•æ˜¯å¦å­˜åœ¨
    const methods = [
      'getSegmentVoiceSettings',
      'updateSegmentVoiceSettings',
      'addTrack',
      'updateTrack'
    ]

    methods.forEach(method => {
      const exists = typeof audioStore[method] === 'function'
      console.log(`ğŸ“‹ ${method}: ${exists ? 'âœ… å­˜åœ¨' : 'âŒ ä¸å­˜åœ¨'}`)
    })

    // æµ‹è¯•2: å¦‚æœæœ‰tracksï¼Œæµ‹è¯•è¯­éŸ³è®¾ç½®åŠŸèƒ½
    if (tracks.value.length > 0) {
      const track = tracks.value[0]
      console.log('ğŸ“¦ å½“å‰tracksæ•°é‡:', tracks.value.length)
      console.log('ğŸ“Š ç¬¬ä¸€ä¸ªtrackçŠ¶æ€:', {
        id: track.id,
        isSegmented: track.isSegmented,
        segmentsCount: track.segments?.length || 0
      })

      if (track.isSegmented && track.segments && track.segments.length > 0) {
        const segment = track.segments[0]
        console.log('ğŸ¯ æµ‹è¯•ç¬¬ä¸€ä¸ªsegmentçš„è¯­éŸ³è®¾ç½®...')

        // å°è¯•è·å–è¯­éŸ³è®¾ç½®
        const currentSettings = audioStore.getSegmentVoiceSettings(track.id, segment.id)
        console.log('ğŸ“¤ è·å–åˆ°çš„è®¾ç½®:', currentSettings)

        // å°è¯•æ›´æ–°è¯­éŸ³è®¾ç½®
        const testSettings = {
          voice: 'test-voice',
          speed: 1.5,
          volume: 0.8,
          pitch: 1.2,
          pauseMarks: [{ charIndex: 5, duration: 1.0 }],
           pronunciationMarks: [{ charIndex: 3, pinyin: 'test' }]
        }

        console.log('ğŸ“¥ å°è¯•ä¿å­˜è®¾ç½®:', testSettings)
        audioStore.updateSegmentVoiceSettings(track.id, segment.id, testSettings)

        // éªŒè¯è®¾ç½®æ˜¯å¦ä¿å­˜æˆåŠŸ
        const updatedSettings = audioStore.getSegmentVoiceSettings(track.id, segment.id)
        console.log('ğŸ” éªŒè¯ä¿å­˜ç»“æœ:', updatedSettings)

        // æ¯”è¾ƒè®¾ç½®æ˜¯å¦ä¸€è‡´ - åªæ¯”è¾ƒæˆ‘ä»¬è®¾ç½®çš„å­—æ®µ
        const fieldsToCheck = ['voice', 'speed', 'volume', 'pitch', 'pauseMarks', 'pronunciationMarks']
        let settingsMatch = true
        let mismatchDetails = []

        for (const field of fieldsToCheck) {
          const testValue = JSON.stringify(testSettings[field])
          const updatedValue = JSON.stringify(updatedSettings[field])

          if (testValue !== updatedValue) {
            settingsMatch = false
            mismatchDetails.push(`${field}: æœŸæœ› ${testValue}, å®é™… ${updatedValue}`)
          }
        }

        if (settingsMatch) {
          console.log('ğŸ“Š è®¾ç½®ä¿å­˜âœ… æˆåŠŸ')
        } else {
          console.log('ğŸ“Š è®¾ç½®ä¿å­˜âŒ å¤±è´¥')
          console.log('âŒ ä¸åŒ¹é…çš„å­—æ®µ:', mismatchDetails)
        }

        return {
          success: settingsMatch,
          originalSettings: currentSettings,
          testSettings,
          updatedSettings
        }
      } else {
        console.log('âš ï¸ æ²¡æœ‰åˆ†æ®µæ•°æ®å¯ä¾›æµ‹è¯•')
        return { success: false, reason: 'æ²¡æœ‰åˆ†æ®µæ•°æ®' }
      }
    } else {
      console.log('âš ï¸ æ²¡æœ‰trackæ•°æ®å¯ä¾›æµ‹è¯•')
      return { success: false, reason: 'æ²¡æœ‰trackæ•°æ®' }
    }
  }

   ;(window as any).debugMarks = (trackId: string, segmentId: string) => {
    console.log('ğŸ” å¼€å§‹è°ƒè¯•æ ‡è®°æ•°æ®...')
    console.log('å‚æ•°:', { trackId, segmentId })

    // 1. ç›´æ¥ä»storeæ£€æŸ¥
    const storeData = audioStore.getSegmentVoiceSettings(trackId, segmentId)
    console.log('ğŸ“¦ Storeä¸­çš„å®Œæ•´æ•°æ®:', storeData)
    console.log('ğŸ“¦ Storeä¸­çš„pauseMarks:', storeData?.pauseMarks)
    console.log('ğŸ“¦ Storeä¸­çš„pronunciationMarks:', storeData?.pronunciationMarks)
    console.log('ğŸ“¦ pauseMarksç±»å‹:', typeof storeData?.pauseMarks)
    console.log('ğŸ“¦ pronunciationMarksç±»å‹:', typeof storeData?.pronunciationMarks)
    console.log('ğŸ“¦ pauseMarksæ˜¯å¦ä¸ºæ•°ç»„:', Array.isArray(storeData?.pauseMarks))
    console.log('ğŸ“¦ pronunciationMarksæ˜¯å¦ä¸ºæ•°ç»„:', Array.isArray(storeData?.pronunciationMarks))

    // 2. æ£€æŸ¥tracksä¸­çš„åŸå§‹æ•°æ®
    const track = tracks.value.find(t => t.id === trackId)
    const segment = track?.segments?.find(s => s.id === segmentId)
    console.log('ğŸ“Š SegmentåŸå§‹æ•°æ®:', segment)
    console.log('ğŸ“Š Segmentä¸­çš„pauseMarks:', segment?.pauseMarks)
    console.log('ğŸ“Š Segmentä¸­çš„pronunciationMarks:', segment?.pronunciationMarks)

    // 3. æ¨¡æ‹ŸhandleOpenVoiceEditorçš„é€»è¾‘
    if (track && segment && storeData) {
      const testSelectedSegment = {
        ...segment,
        voice: storeData.voice || segment.voice,
        speed: storeData.speed || segment.speed,
        pitch: storeData.pitch || segment.pitch,
        volume: storeData.volume || segment.volume,
        ssml: storeData.ssml || segment.ssml,
        pauseMarks: storeData.pauseMarks || [],
        pronunciationMarks: storeData.pronunciationMarks || []
      }
      console.log('ğŸ§ª æ¨¡æ‹Ÿåˆå¹¶ç»“æœ:', testSelectedSegment)
      console.log('ğŸ§ª æ¨¡æ‹Ÿåˆå¹¶åpauseMarks:', testSelectedSegment.pauseMarks)
      console.log('ğŸ§ª æ¨¡æ‹Ÿåˆå¹¶åpronunciationMarks:', testSelectedSegment.pronunciationMarks)
    }

    return {
      storeData,
      segmentData: segment,
      trackFound: !!track,
      segmentFound: !!segment
    }
  }

  console.log('ğŸ‰ AudioStoreå·²æš´éœ²åˆ°windowå¯¹è±¡')
  console.log('ğŸ“š å¯ä»¥åœ¨æ§åˆ¶å°è¿è¡Œæµ‹è¯•:')
  console.log('  - window.testAudioStore() - åŸºç¡€åŠŸèƒ½æµ‹è¯•')
  console.log('  - window.testVoiceSettingsFlow() - è¯­éŸ³è®¾ç½®åˆ‡æ¢æµ‹è¯•')
  console.log('  - window.debugMarks(trackId, segmentId) - è°ƒè¯•æ ‡è®°æ•°æ®ä¸¢å¤±é—®é¢˜')

  // æ£€æŸ¥ç°æœ‰çš„åˆ†å¥æ•°æ®å¹¶åˆå§‹åŒ–gaps
  tracks.value.forEach((track, index) => {

    if (track.isSegmented && track.segments && track.segments.length > 1) {
      if (!track.gaps || track.gaps.length === 0) {
        audioStore.initializeGaps(track.id)
      } else {
        // æ£€æŸ¥ç°æœ‰gapsæ˜¯å¦æœ‰NaNå€¼
        let hasNaN = false
        track.gaps.forEach((gap, gapIndex) => {
          if (isNaN(gap.duration)) {
            gap.duration = 1
            hasNaN = true
          }
        })
        if (hasNaN) {
        }
      }
    }
  })

})
</script>

<style scoped>
.editor {
  height: 100vh;
  background-color: #1a1a1a;
  color: white;
}

.editor-container {
  height: 100%;
  display: flex;
  flex-direction: column;
}

/* å“åº”å¼è®¾è®¡ */
@media (max-width: 768px) {
  .editor-sidebar {
    width: 60px !important;
  }

  .sidebar-content {
    display: none;
  }
}
</style>